{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17533,"status":"ok","timestamp":1736146085400,"user":{"displayName":"Israt Mouri","userId":"17729180794010364471"},"user_tz":-360},"id":"PaOkiimBIHdE","outputId":"4ad3c5c5-1ad7-4468-c7f2-b4bcce3d973f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Installing CLIP...\n","Number of GPUs available: 1\n","Using GPU: cuda:0\n","Devices: ['cuda:0']\n","Number of CPU cores available: 12\n","THREAD_NUMBER set to: 2\n","Mon Jan  6 06:48:04 2025       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   31C    P0              41W / 400W |      5MiB / 40960MiB |      0%      Default |\n","|                                         |                      |             Disabled |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|  No running processes found                                                           |\n","+---------------------------------------------------------------------------------------+\n","\n","Architecture:                         x86_64\n","CPU op-mode(s):                       32-bit, 64-bit\n","Address sizes:                        46 bits physical, 48 bits virtual\n","Byte Order:                           Little Endian\n","CPU(s):                               12\n","On-line CPU(s) list:                  0-11\n","Vendor ID:                            GenuineIntel\n","Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n","CPU family:                           6\n","Model:                                85\n","Thread(s) per core:                   2\n","Core(s) per socket:                   6\n","Socket(s):                            1\n","Stepping:                             7\n","BogoMIPS:                             4400.40\n","Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat avx512_vnni md_clear arch_capabilities\n","Hypervisor vendor:                    KVM\n","Virtualization type:                  full\n","L1d cache:                            192 KiB (6 instances)\n","L1i cache:                            192 KiB (6 instances)\n","L2 cache:                             6 MiB (6 instances)\n","L3 cache:                             38.5 MiB (1 instance)\n","NUMA node(s):                         1\n","NUMA node0 CPU(s):                    0-11\n","Vulnerability Gather data sampling:   Not affected\n","Vulnerability Itlb multihit:          Not affected\n","Vulnerability L1tf:                   Not affected\n","Vulnerability Mds:                    Not affected\n","Vulnerability Meltdown:               Not affected\n","Vulnerability Mmio stale data:        Vulnerable\n","Vulnerability Reg file data sampling: Not affected\n","Vulnerability Retbleed:               Vulnerable\n","Vulnerability Spec rstack overflow:   Not affected\n","Vulnerability Spec store bypass:      Vulnerable\n","Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n","Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Vulnerable; BHI: Vulnerable (Syscall hardening enabled)\n","Vulnerability Srbds:                  Not affected\n","Vulnerability Tsx async abort:        Vulnerable\n","\n"]}],"source":["import os\n","os.environ['CUBLAS_WORKSPACE_CONFIG'] = \":4096:8\"  # or \":16:8\"\n","\n","import sys\n","import subprocess\n","\n","def install(package):\n","    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n","\n","# Function to attempt to import a module, and install it if not present\n","def try_import(module_name, package_name=None):\n","    try:\n","        module = __import__(module_name)\n","        return module\n","    except ImportError:\n","        if package_name is None:\n","            package_name = module_name\n","        print(f\"Installing {package_name}...\")\n","        install(package_name)\n","        module = __import__(module_name)\n","        return module\n","\n","# Standard library imports (no need to install)\n","import logging\n","from datetime import datetime\n","from copy import deepcopy\n","import gc\n","import random\n","import time\n","from concurrent.futures import ThreadPoolExecutor, as_completed\n","from typing import Iterable, Union, Optional\n","\n","# Third-party imports\n","torch = try_import('torch')\n","torchvision = try_import('torchvision')\n","import torchvision.transforms as transforms\n","import torch.optim as optim\n","import numpy as np\n","np = try_import('numpy')\n","# Import torch.nn as nn\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, random_split, Subset, Dataset\n","import torchvision.models as models\n","from torch.nn.functional import tanh, softmax\n","\n","from torchvision.datasets import utils\n","from PIL import Image\n","import os.path\n","import shutil\n","\n","\n","# sklearn imports\n","sklearn = try_import('sklearn', 'scikit-learn')\n","from sklearn.model_selection import train_test_split\n","from sklearn.cluster import AgglomerativeClustering, KMeans\n","from sklearn.metrics import silhouette_score\n","from sklearn.metrics.pairwise import cosine_distances,euclidean_distances\n","from sklearn.metrics import pairwise_distances\n","\n","\n","# Other third-party imports\n","plt = try_import('matplotlib.pyplot', 'matplotlib')\n","pd = try_import('pandas')\n","\n","# Install and import CLIP\n","try:\n","    import clip\n","except ImportError:\n","    print(\"Installing CLIP...\")\n","    install('git+https://github.com/openai/CLIP.git')\n","    import clip\n","\n","\n","\n","# torch.use_deterministic_algorithms(True, warn_only=True)\n","torch.manual_seed(0)\n","\n","# Device configuration\n","# Get the number of available GPUs\n","num_gpus = torch.cuda.device_count()\n","print(f\"Number of GPUs available: {num_gpus}\")\n","\n","# If GPUs are available, choose the desired device index (within the available range)\n","# Otherwise, default to CPU\n","if num_gpus > 0:\n","    desired_gpu_index = 3  # This is the index you originally wanted\n","    device_index = min(desired_gpu_index, num_gpus - 1)  # Clamp to available range\n","    device = torch.device(f\"cuda:{device_index}\")\n","    torch.cuda.set_device(device)  # Set the device\n","    print(f\"Using GPU: {device}\")\n","else:\n","    device = torch.device(\"cpu\")\n","    print(\"No GPUs available, using CPU.\")\n","\n","\n","#set devices to multiple GPUs\n","unwanted_device_indices = []\n","available_device_indices = list(range(num_gpus))\n","devices = [f'cuda:{i}' for i in available_device_indices if i not in unwanted_device_indices]\n","if not devices:\n","    devices = ['cpu']\n","    # raise RuntimeError(\"Desired GPUs are not available.\")\n","print(f\"Devices: {devices}\")\n","\n","\n","\n","\n","import multiprocessing\n","\n","# Get the number of available CPU cores\n","num_cores = multiprocessing.cpu_count()\n","\n","# Set THREAD_NUMBER to the number of CPU cores\n","THREAD_NUMBER = min(num_cores, 2*(len(devices)))\n","# THREAD_NUMBER = 20 # num_cores\n","\n","print(f\"Number of CPU cores available: {num_cores}\")\n","print(f\"THREAD_NUMBER set to: {THREAD_NUMBER}\")\n","\n","\n","\n","# Check GPU information\n","def check_gpu():\n","    try:\n","        gpu_info = subprocess.check_output(['nvidia-smi']).decode('utf-8')\n","        print(gpu_info)\n","    except Exception as e:\n","        print('Not connected to a GPU or nvidia-smi not found.')\n","\n","check_gpu()\n","\n","# Check CPU information\n","def check_cpu():\n","    try:\n","        cpu_info = subprocess.check_output(['lscpu']).decode('utf-8')\n","        print(cpu_info)\n","    except Exception as e:\n","        print('Could not retrieve CPU information.')\n","\n","check_cpu()"]},{"cell_type":"markdown","metadata":{"id":"TPXgVfNmnhES"},"source":["# Function to determine the data root directory"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20926,"status":"ok","timestamp":1736146106324,"user":{"displayName":"Israt Mouri","userId":"17729180794010364471"},"user_tz":-360},"id":"_QYFIElVnecH","outputId":"413f7524-a6b0-42d7-c3b9-df56390743c8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Function to determine the data root directory\n","def get_data_root():\n","    if 'COLAB_GPU' in os.environ:\n","        # Mount Google Drive if needed\n","        from google.colab import drive\n","        drive.mount('/content/drive')\n","        data_root = '/content/drive/MyDrive/PhD/XFED result/Result XFED log/colab output/'\n","    else:\n","        data_root = './data/'\n","    return data_root\n","\n","# Get the appropriate data root directory\n","data_root = get_data_root()"]},{"cell_type":"markdown","metadata":{"id":"TLIncX6sIy49"},"source":["# Model Definition for different datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7zE8s1djJkuN"},"outputs":[],"source":["class FashionMNISTAlexNet(nn.Module):\n","    def __init__(self):\n","        super(FashionMNISTAlexNet, self).__init__()\n","        self.features = nn.Sequential(\n","            nn.Conv2d(1, 96, kernel_size=11, stride=4, padding=0),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=3, stride=2),\n","            nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=3, stride=2),\n","            nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=3, stride=2)\n","        )\n","        self.classifier = nn.Sequential(\n","            nn.Linear(256 * 6 * 6, 4096),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(0.5),\n","            nn.Linear(4096, 4096),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(0.5),\n","            nn.Linear(4096, 10),\n","            nn.LogSoftmax(dim=1)\n","        )\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.classifier(x)\n","        return x\n","\n","class FeatureNorm(nn.Module):\n","    def __init__(self, feature_shape):\n","        super().__init__()\n","        self.gamma = nn.Parameter(torch.ones(1))\n","        self.beta = nn.Parameter(torch.zeros(1, feature_shape))\n","\n","    def forward(self, x):\n","        x = torch.einsum('ni, j->ni', x, self.gamma)\n","        x = x + self.beta\n","        return  x\n","\n","class purchase_fully_connected_IN(nn.Module):\n","    def __init__(self, num_classes):\n","        super(purchase_fully_connected_IN, self).__init__()\n","        self.fc1 = nn.Linear(600, 1024, bias=False)  # First layer: input size 600, output size 1024\n","        self.fc2 = nn.Linear(1024, 100, bias=False)  # Second layer: input size 1024, output size 100\n","        self.fc3 = nn.Linear(100, num_classes, bias=False)  # Output layer: input size 100, output size num_classes\n","        self.norm = FeatureNorm(600)\n","\n","    def forward(self, x):\n","        x = self.norm(x)\n","        x = torch.tanh(self.fc1(x))  # Apply tanh activation after the first layer\n","        x = torch.tanh(self.fc2(x))  # Apply tanh activation after the second layer\n","        logits = self.fc3(x)         # Output layer, no activation\n","        return logits\n","\n","class Purchase(torch.utils.data.Dataset):\n","    def __init__(self, root =data_root + 'dataset_purchase',train=True, download=True, transform = None):\n","        self.images = []\n","        self.root = root\n","        self.targets = []\n","        self.train = train\n","        self.download = download\n","        self.transform = transform\n","\n","        x_train, x_test, y_train, y_test = self._train_test_split()\n","\n","        if self.train:\n","            self._setup_dataset(x_train, y_train)\n","        else:\n","            self._setup_dataset(x_test, y_test)\n","\n","    def _train_test_split(self):\n","        df = pd.read_csv(self.root)\n","\n","        img_names = df.iloc[:, 1:].to_numpy(dtype='f')\n","        img_label = df.iloc[:, 0].to_numpy()-1\n","        x_train,x_test, y_train, y_test = train_test_split(img_names, img_label, train_size=0.8,\n","                                                            random_state=1)\n","        return x_train, x_test, y_train, y_test\n","\n","    def _setup_dataset(self, x, y):\n","            self.images = x\n","            self.targets = y\n","\n","    def __len__(self): # Added the __len__ method\n","        return len(self.images)\n","\n","    def __getitem__(self, item):\n","        img = self.images[item]\n","        label = self.targets[item]\n","        return img, label\n","\n","class ThreeLayerDNN(nn.Module):\n","    def __init__(self, input_size, hidden_size, output_size):\n","        super(ThreeLayerDNN, self).__init__()\n","        self.flatten = nn.Flatten()\n","        self.fc1 = nn.Linear(input_size, hidden_size)\n","        self.relu1 = nn.ReLU()\n","        self.fc2 = nn.Linear(hidden_size, hidden_size)\n","        self.relu2 = nn.ReLU()\n","        self.fc3 = nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, x):\n","        x = self.flatten(x)\n","        x = self.fc1(x)\n","        x = self.relu1(x)\n","        x = self.fc2(x)\n","        x = self.relu2(x)\n","        x = self.fc3(x)\n","        return x\n","\n","class FourLayerDNN(nn.Module):\n","    def __init__(self):\n","        super(FourLayerDNN, self).__init__()\n","        # Flatten the input image\n","        self.flatten = nn.Flatten()\n","        # Define the fully connected layers\n","        self.fc1 = nn.Linear(3 * 32 * 32, 1024)\n","        self.relu1 = nn.ReLU()\n","        self.fc2 = nn.Linear(1024, 512)\n","        self.relu2 = nn.ReLU()\n","        self.fc3 = nn.Linear(512, 256)\n","        self.relu3 = nn.ReLU()\n","        self.fc4 = nn.Linear(256, 10)  # Output layer for 10 classes\n","\n","    def forward(self, x):\n","        x = self.flatten(x)\n","        x = self.relu1(self.fc1(x))\n","        x = self.relu2(self.fc2(x))\n","        x = self.relu3(self.fc3(x))\n","        x = self.fc4(x)\n","        return x\n","\n","class InputNorm(nn.Module):\n","    def __init__(self, num_channel, num_feature):\n","        super().__init__()\n","        self.num_channel = num_channel\n","        self.gamma = nn.Parameter(torch.ones(num_channel))\n","        self.beta = nn.Parameter(torch.zeros(num_channel, num_feature, num_feature))\n","    def forward(self, x):\n","        if self.num_channel == 1:\n","            x = self.gamma*x\n","            x = x + self.beta\n","            return  x\n","        if self.num_channel == 3:\n","            return torch.einsum('...ijk, i->...ijk', x, self.gamma) + self.beta\n","\n","class mnist_fully_connected_IN(nn.Module):\n","    def __init__(self,num_classes):\n","        super(mnist_fully_connected_IN, self).__init__()\n","        self.hidden1 = 600\n","        self.hidden2 = 100\n","        self.fc1 = nn.Linear(28 * 28, self.hidden1, bias=False)\n","        self.fc2 = nn.Linear(self.hidden1, self.hidden2, bias=False)\n","        self.fc3 = nn.Linear(self.hidden2, num_classes, bias=False)\n","        self.relu = nn.ReLU(inplace=False)\n","        self.norm = InputNorm(1, 28)\n","\n","    def forward(self,x):\n","        x = self.norm(x)\n","        x = x.view(-1, 28 * 28)\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        logits = self.fc3(x)\n","        return logits\n","\n","class CHMNISTDataset(Dataset):\n","    def __init__(self, image_folder, transform=None):\n","        self.image_folder = image_folder\n","        self.transform = transform\n","        self.image_paths = [os.path.join(image_folder, img) for img in os.listdir(image_folder)]\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        image_path = self.image_paths[idx]\n","        image = Image.open(image_path).convert('L')  # Convert to grayscale\n","        label = int(image_path.split('_')[-1].split('.')[0])  # Assuming label is in filename\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, label\n","\n","class LogisticRegressionModel(nn.Module):\n","    def __init__(self, input_dim):\n","        super(LogisticRegressionModel, self).__init__()\n","        # Define the linear layer for logistic regression\n","        self.linear = nn.Linear(input_dim, 1)\n","\n","    def forward(self, x):\n","        # Apply the linear layer and then the sigmoid activation\n","        out = torch.sigmoid(self.linear(x))\n","        return out\n","\n","class FEMNISTDataset(torchvision.datasets.MNIST):\n","    def __init__(self, root, train=True, transform=None, target_transform=None, download=False):\n","        super(torchvision.datasets.MNIST, self).__init__(root, transform=transform, target_transform=target_transform)\n","        self.download = download\n","        self.download_link = 'https://media.githubusercontent.com/media/GwenLegate/femnist-dataset-PyTorch/main/femnist.tar.gz'\n","        self.file_md5 = 'a8a28afae0e007f1acb87e37919a21db'\n","        self.train = train\n","        self.root = root\n","        self.training_file = f'{self.root}/FEMNIST/processed/femnist_train.pt'\n","        self.test_file = f'{self.root}/FEMNIST/processed/femnist_test.pt'\n","        self.user_list = f'{self.root}/FEMNIST/processed/femnist_user_keys.pt'\n","\n","        if not os.path.exists(f'{self.root}/FEMNIST/processed/femnist_test.pt') \\\n","                or not os.path.exists(f'{self.root}/FEMNIST/processed/femnist_train.pt'):\n","            if self.download:\n","                self.dataset_download()\n","            else:\n","                raise RuntimeError('Dataset not found, set parameter download=True to download')\n","\n","        if self.train:\n","            data_file = self.training_file\n","        else:\n","            data_file = self.test_file\n","\n","        data_targets_users = torch.load(data_file)\n","        self.data, self.targets, self.users = torch.Tensor(data_targets_users[0]), torch.Tensor(data_targets_users[1]), data_targets_users[2]\n","        self.user_ids = torch.load(self.user_list)\n","\n","    def __getitem__(self, index):\n","        img, target = self.data[index], int(self.targets[index])\n","\n","        # Reshape the flattened image to 28x28\n","        img = img.view(28, 28).numpy().astype(np.uint8)\n","\n","        # Convert to PIL Image in grayscale mode\n","        img = Image.fromarray(img, mode='L')\n","\n","        if self.transform is not None:\n","            img = self.transform(img)\n","        if self.target_transform is not None:\n","            target = self.target_transform(target)\n","        return img, target  # Return only img and target\n","\n","    def dataset_download(self):\n","        paths = [f'{self.root}/FEMNIST/raw/', f'{self.root}/FEMNIST/processed/']\n","        for path in paths:\n","            if not os.path.exists(path):\n","                os.makedirs(path)\n","\n","        # download files\n","        filename = self.download_link.split('/')[-1]\n","        utils.download_and_extract_archive(self.download_link, download_root=f'{self.root}/FEMNIST/raw/', filename=filename, md5=self.file_md5)\n","\n","        files = ['femnist_train.pt', 'femnist_test.pt', 'femnist_user_keys.pt']\n","        for file in files:\n","            # move to processed dir\n","            shutil.move(os.path.join(f'{self.root}/FEMNIST/raw/', file), f'{self.root}/FEMNIST/processed/')"]},{"cell_type":"markdown","metadata":{"id":"2o1uD2lVKAqO"},"source":["Loading Model for different datasets (FashionMNIST, CIFAR-10, PURCHASE, MNIST, EMNIST, CIFAR-100)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tGc5WAzrKNDl"},"outputs":[],"source":["from sklearn.preprocessing import LabelEncoder, StandardScaler\n","class SymbiPredictDataset(Dataset):\n","    def __init__(self, data, labels):\n","        self.data = data\n","        self.labels = labels\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        features = torch.tensor(self.data[idx], dtype=torch.float32)\n","        label = torch.tensor(self.labels[idx], dtype=torch.long)\n","        return features, label\n","\n","class TabularNet(nn.Module):\n","    def __init__(self, input_dim, num_classes):\n","        super(TabularNet, self).__init__()\n","        self.fc1 = nn.Linear(input_dim, 128)\n","        self.fc2 = nn.Linear(128, 64)\n","        self.fc3 = nn.Linear(64, num_classes)\n","\n","    def forward(self, x):\n","        x = torch.relu(self.fc1(x))\n","        x = torch.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","\n","def load_model(dataset: str):\n","    \"\"\"Load and prepare the model and datasets based on the given dataset name.\"\"\"\n","    if dataset == 'FashionMNIST':\n","        transform = transforms.Compose([\n","            transforms.Resize((227, 227)),\n","            transforms.ToTensor(),\n","            transforms.Normalize((0.1307,), (0.3081,))\n","        ])\n","        train_data = torchvision.datasets.FashionMNIST(root=data_root, train=True, download=True, transform=transform)\n","        test_data = torchvision.datasets.FashionMNIST(root=data_root, train=False, download=True, transform=transform)\n","        testloader = DataLoader(test_data, batch_size=64, shuffle=False, num_workers=8)\n","        classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot')\n","        model = FashionMNISTAlexNet().to(device)\n","\n","    elif dataset == 'CIFAR10':\n","        transform = transforms.Compose([\n","            transforms.Resize(256),\n","            transforms.CenterCrop(224),\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","        ])\n","        train_data = torchvision.datasets.CIFAR10(root=data_root, train=True, download=True, transform=transform)\n","        test_data = torchvision.datasets.CIFAR10(root=data_root, train=False, download=True, transform=transform)\n","        testloader = DataLoader(test_data, batch_size=64, shuffle=False, num_workers=8)\n","        classes = ('Airplane', 'Car', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck')\n","\n","        model = models.alexnet(pretrained=True)\n","        model.classifier[1] = nn.Linear(9216, 4096)\n","        model.classifier[4] = nn.Linear(4096, 1024)\n","        model.classifier[6] = nn.Linear(1024, 10)\n","        model = model.to(device)\n","\n","    elif dataset == 'PURCHASE':\n","        train_data = Purchase(train=True, download=True)\n","        test_data = Purchase(train=False, download=True)\n","        testloader = DataLoader(test_data, batch_size=64, shuffle=False, num_workers=8)\n","\n","        model = purchase_fully_connected_IN(100).to(device)\n","\n","    elif dataset == 'CHMNIST':\n","        transform = transforms.Compose([\n","            transforms.ToTensor(),\n","            transforms.Normalize((0.5,), (0.5,))\n","        ])\n","        train_data = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform) # Assuming CHMNIST is similar to MNIST\n","        test_data = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n","        trainloader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True, num_workers=2)\n","        testloader = torch.utils.data.DataLoader(test_data, batch_size=64, shuffle=False, num_workers=2)\n","        model=models.mobilenet_v2(pretrained=True).to(device)\n","        model.features[0][0] = nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","\n","\n","\n","    elif dataset == 'EMNIST':\n","        transform = transforms.Compose([\n","            transforms.Resize((28, 28)),\n","            transforms.ToTensor(),\n","            transforms.Normalize((0.5,), (0.5,))\n","        ])\n","\n","        train_data = torchvision.datasets.EMNIST(root='./data', split='byclass', train=True, download=True, transform=transform)\n","        test_data = torchvision.datasets.EMNIST(root='./data', split='byclass', train=False, download=True, transform=transform)\n","        testloader = DataLoader(test_data, batch_size=64, shuffle=False, num_workers=8)\n","\n","        model = ThreeLayerDNN(input_size=28 * 28, hidden_size=512, output_size=62).to(device)\n","\n","    elif dataset == 'MNIST':\n","        # Define transformation for MNIST\n","        transform = transforms.Compose([\n","            transforms.ToTensor(),        # Convert image to PyTorch tensor\n","            transforms.Normalize((0.5,), (0.5,))  # Normalize grayscale values to [-1, 1]\n","        ])\n","\n","        # Load the MNIST dataset (\"ByClass\" split as an example)\n","        train_data = torchvision.datasets.MNIST(root=data_root, train=True, download=True, transform=transform)\n","        test_data = torchvision.datasets.MNIST(root=data_root, train=False, download=True, transform=transform)\n","        testloader = DataLoader(test_data, batch_size=64, shuffle=False, num_workers=8)\n","\n","        # Load the pre-trained VGG16 model\n","        model = mnist_fully_connected_IN(10).to(device)\n","\n","    elif dataset == 'CIFAR100':\n","        # Define the transformation for the dataset (matching CLIP preprocessing)\n","        transform = transforms.Compose([\n","            transforms.Resize((224, 224)),  # CLIP expects 224x224 input\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711]),\n","        ])\n","\n","        # Load the CIFAR100 dataset\n","        train_data = torchvision.datasets.CIFAR100(root=data_root, train=True, download=True, transform=transform)\n","        test_data = torchvision.datasets.CIFAR100(root=data_root, train=False, download=True, transform=transform)\n","\n","        # Create DataLoader for train and test sets\n","        trainloader = DataLoader(train_data, batch_size=32, shuffle=True, num_workers=8)\n","        testloader = DataLoader(test_data, batch_size=32, shuffle=False, num_workers=8)\n","\n","        # Define the class labels for CIFAR100\n","        classes = [str(i) for i in range(100)]  # CIFAR100 has 100 classes\n","\n","        # Load the CLIP model from OpenAI\n","        model_clip, preprocess = clip.load(\"ViT-B/32\", device=device)\n","\n","        # Convert CLIP model to float32 to match other layers and data\n","        model_clip = model_clip.float()\n","\n","        # Freeze the CLIP model's parameters (we're only training the classifier)\n","        for param in model_clip.parameters():\n","            param.requires_grad = False\n","\n","        # Define a simple 1-layer DNN model on top of CLIP features\n","        class CLIP_DNN(nn.Module):\n","            def __init__(self, clip_model, num_classes=100):\n","                super(CLIP_DNN, self).__init__()\n","                self.clip_model = clip_model\n","                self.fc = nn.Linear(512, num_classes)  # CLIP ViT-B/32 gives 512-dimensional features\n","\n","            def forward(self, images):\n","                with torch.no_grad():\n","                    # Extract image features using CLIP's image encoder (cast to float32)\n","                    image_features = self.clip_model.encode_image(images).float()\n","                return self.fc(image_features)\n","\n","        # Initialize the model\n","        model = CLIP_DNN(model_clip, num_classes=100)\n","\n","        # Move the model to the device (GPU or CPU)\n","        model = model.to(device)\n","\n","\n","    elif dataset == 'SYMBIPREDICT':\n","        # Load the CSV file\n","        csv_file = os.path.join(data_root, 'symbipredict_2022.csv')\n","        df = pd.read_csv(csv_file)\n","\n","        # Encode target labels\n","        label_encoder = LabelEncoder()\n","        df['prognosis'] = label_encoder.fit_transform(df['prognosis'])\n","\n","        # Separate features and labels\n","        X = df.drop(columns=['prognosis']).values\n","        y = df['prognosis'].values\n","\n","        # Standardize features\n","        scaler = StandardScaler()\n","        X = scaler.fit_transform(X)\n","\n","        # Split into training and testing sets\n","        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","        # Create Dataset instances\n","        train_data = SymbiPredictDataset(X_train, y_train)\n","        test_data = SymbiPredictDataset(X_test, y_test)\n","\n","        # DataLoader for test data only\n","        testloader = DataLoader(test_data, batch_size=64, shuffle=False, num_workers=4)\n","\n","        # Define model\n","        input_dim = X_train.shape[1]\n","        num_classes = len(label_encoder.classes_)\n","        model = TabularNet(input_dim=input_dim, num_classes=num_classes).to(device)\n","\n","    else:\n","        raise ValueError(\"Dataset not supported\")\n","\n","    return model, train_data, testloader\n"]},{"cell_type":"markdown","metadata":{"id":"htLtRlfGK1ku"},"source":["Aggregation Rules (\n","FedAvg / Mean,\n","Median,\n","Trimmed Mean,\n","Multi-Krum,\n","Clipped Clustering,\n","SignGuard)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qwM4nYoULKwO"},"outputs":[],"source":["def tr_mean(all_updates: torch.Tensor) -> torch.Tensor:\n","    \"\"\"Apply Trimmed Mean aggregation with 20% assumed attackers.\"\"\"\n","    # tmp = all_updates\n","    # all_updates = all_updates.cpu()\n","    # del tmp\n","    # torch.cuda.empty_cache()\n","    sorted_updates = torch.sort(all_updates, dim=0)[0]\n","    num_clients = len(all_updates)\n","    n_attackers = round(0.2 * num_clients)\n","    if n_attackers != 0 and 2 * n_attackers < num_clients:\n","        ret = torch.mean(sorted_updates[n_attackers:-n_attackers], dim=0)\n","        # print(\"sorted_updates\", sorted_updates)\n","        # print(\"num_clients\", num_clients)\n","        # print(\"n_attackers\", n_attackers)\n","        # print(\"ret\", ret)\n","        return ret\n","    return torch.mean(sorted_updates, dim=0).to(device)\n","\n","def multi_krum_optimized(local_updates: torch.Tensor):\n","    \"\"\"\n","    Implements a memory-optimized version of the Multi-Krum aggregation rule with explicit deletion of local variables.\n","    Parameters:\n","    - local_updates: A tensor of shape (num_clients, num_params) containing the flattened model updates from each client.\n","    Returns:\n","    - The aggregated model update as a tensor of shape (num_params,).\n","    \"\"\"\n","    num_clients = local_updates.size(0)\n","    byzantine_client_num = int(num_clients * 0.2)  # Assuming 20% are byzantine clients\n","    krum_limit = num_clients - byzantine_client_num - 2\n","\n","    # Instead of computing a full pairwise distance matrix, compute distances incrementally\n","    scores = torch.zeros(num_clients)\n","\n","    for i in range(num_clients):\n","        # Compute the squared L2 distances between client `i` and all other clients\n","        distances = torch.sum((local_updates - local_updates[i]) ** 2, dim=1)\n","\n","        # Sort distances and ignore the first distance (which is 0, i.e., distance to itself)\n","        sorted_distances, _ = torch.sort(distances)\n","\n","        # Sum the smallest `krum_limit` distances (ignore the first one)\n","        scores[i] = torch.sum(sorted_distances[1:krum_limit + 1])\n","\n","        # Explicitly delete large tensors to free memory\n","        del distances, sorted_distances\n","\n","    # Select the indices of the `krum_limit` clients with the lowest scores\n","    selected_indices = torch.topk(-scores, krum_limit, largest=True).indices\n","\n","    # Average the updates of the selected clients\n","    aggregated_update = torch.mean(local_updates[selected_indices], dim=0)\n","\n","    # Clean up memory before returning\n","    del scores, local_updates\n","\n","    return aggregated_update, selected_indices\n","\n","def clip_tensor_norm_(\n","    parameters: Union[torch.Tensor, Iterable[torch.Tensor]],\n","    max_norm: float,\n","    norm_type: float = 2.0,\n","    error_if_nonfinite: bool = False,\n",") -> torch.Tensor:\n","    if isinstance(parameters, torch.Tensor):\n","        parameters = [parameters]\n","\n","    max_norm = float(max_norm)\n","    norm_type = float(norm_type)\n","\n","    if len(parameters) == 0:\n","        return torch.tensor(0.0)\n","\n","    device = parameters[0].device\n","\n","    if norm_type ==  float('inf'):\n","        norms = [p.detach().abs().max().to(device) for p in parameters]\n","        total_norm = norms[0] if len(norms) == 1 else torch.max(torch.stack(norms))\n","    else:\n","        total_norm = torch.norm(\n","            torch.cat(\n","                [\n","                    p.detach().view(-1).to(device)\n","                    for p in parameters\n","                    if p.dtype != torch.int64\n","                ]\n","            ),\n","            norm_type,\n","        )\n","\n","    if error_if_nonfinite and torch.logical_or(total_norm.isnan(), total_norm.isinf()):\n","        raise RuntimeError(\n","            f\"The total norm of order {norm_type} for gradients from \"\n","            \"`parameters` is non-finite, so it cannot be clipped. To disable \"\n","            \"this error and scale the gradients by the non-finite norm anyway, \"\n","            \"set `error_if_nonfinite=False`\"\n","        )\n","\n","    clip_coef = max_norm / (total_norm + 1e-6)\n","    clip_coef_clamped = torch.clamp(clip_coef, max=1.0)\n","\n","    for p in parameters:\n","        if p.dtype != torch.int64:\n","            p.mul_(clip_coef_clamped.to(p.device))\n","\n","def Clippedclustering(updates: torch.Tensor):\n","    tau = 1e5\n","    l2norm_his = []\n","\n","    # Calculate L2 norms in a single operation\n","    l2norms = [torch.norm(update).item() for update in updates]\n","    l2norm_his.extend(l2norms)\n","\n","    threshold = np.median(l2norm_his)\n","    threshold = min(threshold, tau)\n","\n","    # Clip tensor norms above the threshold\n","    for idx, l2 in enumerate(l2norms):\n","        if l2 > threshold:\n","            clip_tensor_norm_(updates[idx], threshold)\n","\n","    num = len(updates)\n","\n","    dis_max = 1 - torch.mm(\n","        updates, updates.t()\n","    ).cpu().numpy()  # Convert to numpy for AgglomerativeClustering\n","\n","    # Handle boundary conditions for distance matrix\n","    dis_max = np.where(np.isinf(dis_max), 2.0, np.where(np.isnan(dis_max), 2.0, dis_max))\n","\n","    # Hierarchical clustering\n","    clustering = AgglomerativeClustering(\n","        metric=\"precomputed\", linkage=\"average\", n_clusters=2\n","    )\n","    clustering.fit(dis_max)\n","\n","    flag = 1 if np.sum(clustering.labels_) > num // 2 else 0\n","    S1_idxs = [idx for idx, label in enumerate(clustering.labels_) if label == flag]\n","\n","    # Vectorized feature extraction\n","    num_para = len(updates[0])\n","    feature0 = (updates > 0).float().mean(dim=1)\n","    feature1 = (updates < 0).float().mean(dim=1)\n","    feature2 = (updates == 0).float().mean(dim=1)\n","\n","    features = torch.stack([feature0, feature1, feature2], dim=1).cpu().numpy()\n","\n","    # KMeans clustering\n","    kmeans = KMeans(n_clusters=2, random_state=0).fit(features)\n","    flag = 1 if np.sum(kmeans.labels_) > num // 2 else 0\n","    S2_idxs = [idx for idx, label in enumerate(kmeans.labels_) if label == flag]\n","\n","    # Select intersection of both clustering methods\n","    selected_idxs = list(set(S1_idxs) & set(S2_idxs))\n","\n","    # Return the mean of selected updates\n","    return torch.mean(updates[selected_idxs], dim=0)\n","\n","def SignGuard(updates):\n","    # updates = updates.cpu()\n","\n","    num = updates.shape[0]\n","    # Compute L2 norms across all dimensions except the first\n","    l2norms = torch.norm(updates, dim=tuple(range(1, updates.ndim)))\n","\n","    # Compute the median using torch.median (stays on GPU)\n","    M = torch.median(l2norms)\n","    L = 0.1\n","    R = 3.0\n","\n","    # Create a mask for S1 indices\n","    mask1 = (l2norms >= L * M) & (l2norms <= R * M)\n","    del l2norms, M  # Delete l2norms and M as they're no longer needed\n","    torch.cuda.empty_cache()\n","\n","    # Flatten updates for feature computation\n","    updates_flat = updates.view(updates.shape[0], -1).cpu()\n","    num_para = updates_flat.size(1)\n","\n","    # Compute features using vectorized operations\n","    positive_counts = (updates_flat > 0).sum(dim=1).float() / num_para\n","    negative_counts = (updates_flat < 0).sum(dim=1).float() / num_para\n","    zero_counts = (updates_flat == 0).sum(dim=1).float() / num_para\n","\n","    features = torch.stack([positive_counts, negative_counts, zero_counts], dim=1).cpu().numpy()\n","    del updates_flat, positive_counts, negative_counts, zero_counts  # Clean up\n","    torch.cuda.empty_cache()\n","\n","    # Perform KMeans clustering\n","    kmeans = KMeans(n_clusters=2, random_state=0).fit(features)\n","    labels = kmeans.labels_\n","    del kmeans, features  # Clean up CPU memory\n","\n","    # Convert labels back to a CUDA tensor\n","    labels = torch.from_numpy(labels).to(device)\n","\n","    # Determine the majority cluster\n","    flag = 1 if labels.sum() > num // 2 else 0\n","\n","    # Create a mask for S2 indices\n","    mask2 = (labels == flag)\n","    del labels  # Delete labels as it's no longer needed\n","    torch.cuda.empty_cache()\n","\n","    # Intersection of S1 and S2 indices\n","    inter_mask = mask1 & mask2\n","    del mask1, mask2  # Clean up masks\n","    torch.cuda.empty_cache()\n","\n","    # Select the updates based on the intersection mask\n","    selected_updates = updates[inter_mask]\n","    del updates, inter_mask  # Delete updates and inter_mask\n","    torch.cuda.empty_cache()\n","\n","    # Compute and return the mean of the selected updates\n","    result = torch.mean(selected_updates, dim=0)\n","    del selected_updates  # Clean up selected_updates\n","    torch.cuda.empty_cache()\n","\n","    return result.to(device)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14921,"status":"ok","timestamp":1736146121241,"user":{"displayName":"Israt Mouri","userId":"17729180794010364471"},"user_tz":-360},"id":"CD43V5SHHWwD","outputId":"7493e7ef-3955-44be-8be3-7417c4cb2029"},"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset Statistics:\n","  Number of samples: 60000\n","  Number of classes: 10\n","  Sample count per class:\n","    Label 5: 5421 samples\n","    Label 0: 5923 samples\n","    Label 4: 5842 samples\n","    Label 1: 6742 samples\n","    Label 9: 5949 samples\n","    Label 2: 5958 samples\n","    Label 3: 6131 samples\n","    Label 6: 5918 samples\n","    Label 7: 6265 samples\n","    Label 8: 5851 samples\n"]}],"source":["from collections import Counter\n","\n","def print_dataset_statistics(dataset):\n","    \"\"\"Prints dataset statistics including sample count, classes, and sample count per class.\n","\n","    Args:\n","        dataset: The PyTorch dataset object.\n","    \"\"\"\n","    labels = [label for _, label in dataset]  # Extract all labels\n","    label_counts = Counter(labels)            # Count label occurrences\n","\n","    num_samples = len(dataset)                # Total number of samples\n","    num_classes = len(label_counts)            # Number of unique classes\n","\n","    print(f\"Dataset Statistics:\")\n","    print(f\"  Number of samples: {num_samples}\")\n","    print(f\"  Number of classes: {num_classes}\")\n","    print(f\"  Sample count per class:\")\n","    for label, count in label_counts.items():\n","        print(f\"    Label {label}: {count} samples\")\n","\n","# Example usage:\n","global_model, train_data, testloader = load_model('MNIST')\n","print_dataset_statistics(train_data)  # Assuming 'train_data' is your dataset"]},{"cell_type":"markdown","metadata":{"id":"WgmZW6tziyz6"},"source":["Attacks (XFED)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6ZFitOiQi5uH"},"outputs":[],"source":["def get_mu_pairwise_distance(global_models, MU_MULTIPLIER):\n","    \"\"\"Compute pairwise distance based deviation (mu).\"\"\"\n","    num_models = len(global_models)\n","    # print(\"num_models\", num_models)\n","    # print(\"global_models\", global_models)\n","    if num_models > 1:\n","        if isinstance(global_models, list):\n","            global_models_tensor = torch.vstack(global_models)\n","        else:\n","            global_models_tensor = global_models\n","        # Step 1: Calculate the centroid (mean vector)\n","        centroid = torch.mean(global_models_tensor, dim=0)\n","        # Step 2: Compute the Euclidean distance of each vector from the centroid\n","        # Step 3: Calculate the standard deviation of the distances\n","        distances = torch.norm(global_models_tensor - centroid, dim=1)\n","        std_dev = torch.sqrt(torch.dot(distances, distances) / num_models)\n","        mu = MU_MULTIPLIER * std_dev\n","        return mu\n","    else:\n","        return torch.tensor(0.0)\n","\n","def xfed_c(user_grads, n_attackers, dev_type, len_global, global_model_data, global_models, collab):\n","    if collab == 0:\n","        all_updates = user_grads[:n_attackers]\n","        start_idx = 0\n","    else:\n","        individual_attackers = n_attackers - collab\n","        all_updates = user_grads[individual_attackers:n_attackers]\n","        start_idx = individual_attackers\n","\n","    model_re = torch.mean(all_updates, dim=0).to(device)\n","\n","    if dev_type == 'C_XFED_unit_vec' or dev_type == 'Hybrid_XFED_unit_vec':\n","        deviation = model_re / torch.norm(model_re)\n","    elif dev_type == 'C_XFED_sign' or dev_type == 'Hybrid_XFED_sign':\n","        sgn_vec = torch.sign(model_re)\n","        deviation = sgn_vec / torch.norm(sgn_vec)\n","    elif dev_type == 'C_XFED_std':\n","        deviation = torch.std(all_updates, dim=0)\n","\n","    if len_global > 1:\n","        # print(torch.norm(model_re - global_model_data), get_mu_pairwise_distance(global_models, 3), get_mu_pairwise_distance(all_updates, 1))\n","        # mu = torch.max(torch.norm(model_re - global_model_data), get_mu_pairwise_distance(global_models))\n","        # mu = torch.max(torch.norm(model_re - global_model_data), get_mu_pairwise_distance(all_updates))\n","        mu = get_mu_pairwise_distance(all_updates, 3)\n","    else:\n","        mu = torch.tensor(1.0)\n","\n","     # Calculate delta and malicious updates\n","    deviation *= mu\n","    mal_update = (model_re - deviation)\n","\n","    del model_re, deviation\n","    torch.cuda.empty_cache()\n","\n","\n","    for i in range(start_idx, n_attackers):\n","        tmp = user_grads[i]\n","        user_grads[i] = mal_update\n","        del tmp\n","        torch.cuda.empty_cache()\n","\n","    return user_grads\n","\n","    # Create the final stacked tensor of updates\n","    # Combine the malicious updates with the rest of the user_grads\n","    # mal_updates = mal_update.unsqueeze(0).repeat(n_attackers, *[1 for _ in mal_update.shape])\n","    # return torch.cat((mal_updates, user_grads[n_attackers:]), dim=0)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Dw82zh_Zjslf"},"source":["Attacks( VIRAT, FANG-TR-MEAN, FANG-KRUM, LIE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ynbQi9pnjy-G"},"outputs":[],"source":["\n","def virat_min_max(user_grads, n_attackers, dev_type='VIRAT_unit_vec', epoch = 0, threshold=50):\n","    \"\"\"Implement VIRAT Min-Max attack.\"\"\"\n","    all_updates = user_grads[:n_attackers].to(device)\n","    model_re = torch.mean(all_updates, dim=0).to(device)\n","\n","    if dev_type == 'VIRAT_unit_vec':\n","        deviation = model_re / torch.norm(model_re)\n","    elif dev_type == 'VIRAT_sign':\n","        deviation = torch.sign(model_re)\n","    elif dev_type == 'VIRAT_std':\n","        deviation = torch.std(all_updates, dim=0)\n","    else:\n","        raise ValueError(f\"Unknown deviation type: {dev_type}\")\n","\n","    lamda = torch.tensor([threshold], dtype=torch.float).to(device)\n","    threshold_diff = 1e-5\n","    lamda_fail = lamda.clone()\n","    lamda_succ = torch.tensor(0, dtype=torch.float).to(device)\n","\n","    distances = []\n","    for update in all_updates:\n","        distance = torch.norm((all_updates - update), dim=1) ** 2\n","        distances = distance[None, :] if not len(distances) else torch.cat((distances, distance[None, :]), 0)\n","\n","    max_distance = torch.max(distances)\n","    del distances\n","\n","    while torch.abs(lamda_succ - lamda) > threshold_diff:\n","        mal_update = model_re - lamda * deviation\n","        distance = torch.norm(all_updates - mal_update, dim=1) ** 2\n","        max_d = torch.max(distance)\n","\n","        if max_d <= max_distance:\n","            lamda_succ = lamda.clone()\n","            lamda += lamda_fail / 2\n","        else:\n","            lamda -= lamda_fail / 2\n","\n","        lamda_fail /= 2\n","    print(f\"For round {epoch}, max_distance\", max_distance, \"lamda_succ\", lamda_succ)\n","    mal_update = (model_re - lamda_succ * deviation) #.to('cpu')\n","    mal_updates = mal_update.unsqueeze(0).repeat(n_attackers, *[1 for _ in mal_update.shape])\n","    return torch.cat((mal_updates, user_grads[n_attackers:]), dim=0)\n","\n","def fang_attack_trmean_partial(user_grads, n_attackers):\n","\n","    all_updates = user_grads[:n_attackers]\n","    model_re = torch.mean(all_updates, 0)\n","    model_std = torch.std(all_updates, 0)\n","    deviation = torch.sign(model_re)\n","\n","    max_vector_low = model_re + 3 * model_std\n","    max_vector_hig = model_re + 4 * model_std\n","    min_vector_low = model_re - 4 * model_std\n","    min_vector_hig = model_re - 3 * model_std\n","\n","    max_range = torch.cat((max_vector_low[:,None], max_vector_hig[:,None]), dim=1)\n","    min_range = torch.cat((min_vector_low[:,None], min_vector_hig[:,None]), dim=1)\n","\n","    rand = torch.from_numpy(np.random.uniform(0, 1, [len(deviation), n_attackers])).type(torch.FloatTensor).to(device)\n","\n","    max_rand = torch.stack([max_range[:, 0]] * rand.shape[1]).T + rand * torch.stack([max_range[:, 1] - max_range[:, 0]] * rand.shape[1]).T\n","    min_rand = torch.stack([min_range[:, 0]] * rand.shape[1]).T + rand * torch.stack([min_range[:, 1] - min_range[:, 0]] * rand.shape[1]).T\n","\n","    deviation_sign = (deviation > 0).type(torch.FloatTensor).to(device)\n","\n","    mal_update = (torch.stack([deviation_sign] * max_rand.shape[1]).T * max_rand + torch.stack(\n","        [deviation_sign] * min_rand.shape[1]).T * min_rand).T\n","\n","    return torch.cat((mal_update, user_grads[n_attackers:]), dim=0)\n","\n","def compute_lambda_fang(all_updates, model_re, n_attackers):\n","    distances = []\n","    n_benign, d = all_updates.shape\n","    for update in all_updates:\n","        distance = torch.norm((all_updates - update), dim=1)\n","        distances = distance[None, :] if not len(distances) else torch.cat((distances, distance[None, :]), 0)\n","\n","    distances[distances == 0] = 10000\n","    distances = torch.sort(distances, dim=1)[0]\n","    scores = torch.sum(distances[:, :n_benign - 2 - n_attackers], dim=1)\n","    min_score = torch.min(scores)\n","    term_1 = min_score / ((n_benign - n_attackers - 1) * torch.sqrt(torch.Tensor([d]))[0])\n","    max_wre_dist = torch.max(torch.norm((all_updates - model_re), dim=1)) / (torch.sqrt(torch.Tensor([d]))[0])\n","\n","    return (term_1 + max_wre_dist)\n","\n","def get_malicious_updates_fang(all_updates, model_re, deviation, n_attackers):\n","\n","    lamda = 100 # compute_lambda_fang(all_updates, model_re, n_attackers)\n","    threshold = torch.tensor(1e-5)\n","\n","    mal_updates = []\n","    while lamda > threshold:\n","        mal_update = (model_re - lamda * deviation)\n","\n","        mal_updates = torch.stack([mal_update] * n_attackers)\n","        mal_updates = torch.cat((mal_updates, all_updates), 0)\n","        mal_updates = torch.cat((mal_updates, all_updates), 0)\n","        mal_updates = torch.cat((mal_updates, all_updates), 0)\n","        mal_updates = torch.cat((mal_updates, all_updates), 0)\n","\n","        _, selected_indices = multi_krum_optimized(mal_updates)\n","        # print(f'len {len(mal_updates)}, lamda {lamda}, indices {selected_indices}')\n","\n","        if torch.any(selected_indices < n_attackers):\n","            return mal_update\n","\n","        lamda *= 0.5\n","\n","    if not len(mal_updates):\n","        print(lamda, threshold)\n","        mal_update = (model_re - lamda * deviation)\n","    return mal_update\n","\n","def lie_attack(user_grads, n_attackers, z):\n","\n","    # Stack the gradients for the attackers\n","    all_updates = user_grads[:n_attackers]\n","\n","    # Calculate mean and standard deviation of the attacker's updates\n","    avg = torch.mean(all_updates, dim=0)\n","    std = torch.std(all_updates, dim=0)\n","\n","    # Generate malicious updates\n","    mal_update = avg + z * std\n","\n","    mal_updates = mal_update.unsqueeze(0).repeat(n_attackers, *[1 for _ in mal_update.shape])\n","    return torch.cat((mal_updates, user_grads[n_attackers:]), dim=0)\n"]},{"cell_type":"markdown","metadata":{"id":"_pUsLI-nkkTK"},"source":["Code for calculating Z value"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1736146121242,"user":{"displayName":"Israt Mouri","userId":"17729180794010364471"},"user_tz":-360},"id":"_gUO5IWDkp0n","outputId":"a20967df-53fc-424e-fb05-af315726c44c"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.7\n"]}],"source":["z_values={(50,3):0.69847, (50,5):0.7054, (50,8):0.71904, (50,10):0.72575, (50,12):0.73891, (100,20):0.72907, (40, 8): 0.72575, (100,5):0.69497, (100,10):0.7054, (100,15):0.71566, (100,25):0.74215, (100, 30):0.75804}\n","# z value calculation code to execute lie attack\n","import math\n","# Update the value of m to 10\n","n=100\n","m = 30\n","\n","# Recalculate s and z\n","s = math.floor(n / 2 + 1) - m\n","z = (n - m - s) / (n - m)\n","print(z)"]},{"cell_type":"markdown","metadata":{"id":"AfA9uC4lkvAv"},"source":["Federated Learning Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zXWWAdggktr4"},"outputs":[],"source":["\n","\n","\n","def train_local_model(client_id, client_indices, global_model, train_data, batch_size, criterion, device, optimizer):\n","    sampled_indices = random.sample(client_indices, min(batch_size, len(client_indices)))\n","    sampled_data = Subset(train_data, sampled_indices)\n","    # print(f\"client_id: {client_id}, sampled_indices: {len(sampled_indices)}, sampled_data: {len(sampled_data)}\")\n","    sampled_loader = DataLoader(sampled_data, batch_size=len(sampled_indices), shuffle=False, num_workers=0) # Set batch_size to the length of sampled_data\n","\n","    # Move the model to the assigned GPU device\n","    local_model = deepcopy(global_model).to(device)\n","    if optimizer == 'SGD':\n","        local_optimizer = optim.SGD(local_model.parameters(), lr=0.5, momentum=0.9)\n","    else:\n","        local_optimizer = torch.optim.Adam(local_model.parameters(), lr=0.001)\n","\n","\n","    for inputs, targets in sampled_loader:\n","        inputs, targets = inputs.to(device), targets.to(device)\n","        # print(len(inputs), len(targets))\n","        local_optimizer.zero_grad()\n","        outputs = local_model(inputs)\n","        loss = criterion(outputs, targets)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(local_model.parameters(), max_norm=1.0)\n","        local_optimizer.step()\n","\n","    # Collect model parameters for aggregation\n","    # local_params = torch.cat([param.data.view(-1).cpu() for param in local_model.parameters()])\n","    local_params = torch.cat([param.data.view(-1) for param in local_model.parameters()])\n","\n","\n","    # Cleanup\n","    del local_model, sampled_data, sampled_loader\n","    torch.cuda.empty_cache()\n","\n","    return client_id, local_params\n","\n","\n","def federated_learning(num_clients=10, aggregation='MEAN', n_attackers=3, attack_type='XFED_unit_vec', dataset='CIFAR10', n_round=1000, batch_size=64, optim='SGD', cross_device=True, collab=5):\n","    \"\"\"Main federated learning loop.\"\"\"\n","    global_model, train_data, testloader = load_model(dataset)\n","    criterion = nn.CrossEntropyLoss()\n","\n","    # Step 1: Split the dataset among clients\n","    total_data_size = len(train_data)\n","    client_data_size = total_data_size // num_clients\n","    print(\"client_data_size\", client_data_size)\n","    indices = list(range(total_data_size))\n","    random.shuffle(indices)\n","    clients_data_indices = [indices[i * client_data_size:(i + 1) * client_data_size] for i in range(num_clients)]\n","\n","    global_models, global_model_data = [], []\n","\n","    with ThreadPoolExecutor(max_workers=THREAD_NUMBER) as executor:  # Adjust max_workers based on your system capabilities\n","        for epoch in range(n_round):\n","            global_model.train()\n","            local_models_data_diff = []\n","\n","            # Delete the oldest item if size is greater then 25\n","            if len(global_models) > 4:\n","                del global_models[0]\n","\n","            futures = [\n","                executor.submit(\n","                    train_local_model,\n","                    client_id,\n","                    client_indices,\n","                    global_model,\n","                    train_data,\n","                    batch_size,\n","                    criterion,\n","                    devices[client_id % len(devices)],  # Alternate between 'cuda:0' and 'cuda:1'\n","                    optim\n","                )\n","                for client_id, client_indices in enumerate(clients_data_indices)\n","            ]\n","\n","            # Collect results\n","            for future in as_completed(futures):\n","                client_id, local_params = future.result()\n","                local_models_data_diff.append(local_params)\n","\n","            for i in range(torch.cuda.device_count()):\n","                torch.cuda.set_device(i)\n","                torch.cuda.empty_cache()\n","            if num_gpus > 0:\n","                torch.cuda.set_device(device)\n","            print(f'For round {epoch}, training done')\n","            # time.sleep(30)\n","            local_models_data = torch.stack(local_models_data_diff).to(device)\n","            del local_models_data_diff\n","            gc.collect()\n","\n","            if attack_type.startswith('XFED'):\n","                for local_machine in range(n_attackers):\n","                    if attack_type == 'XFED_unit_vec':\n","                        deviation = local_models_data[local_machine] / torch.norm(local_models_data[local_machine])\n","                    elif attack_type == 'XFED_sign':\n","                        sgn_vec = torch.sign(local_models_data[local_machine])\n","                        deviation = sgn_vec / torch.norm(sgn_vec)\n","                    else:\n","                        raise ValueError(\"Invalid attack type\")\n","\n","                    if len(global_models) > 1:\n","\n","                        # version 1\n","                        # print(f\"\\n\\nFor round {epoch} and advNumber {local_machine}, mu\", torch.norm(local_models_data[local_machine] - global_model_data), get_mu_pairwise_distance(global_models))\n","                        # mu = torch.max(torch.norm(local_models_data[local_machine] - global_model_data), get_mu_pairwise_distance(global_models, MU_MULTIPLIER))\n","\n","                        # version 2\n","                        # mu = torch.norm(local_models_data[local_machine] - global_model_data)\n","                        # print(f\"For round {epoch} and advNumber {local_machine}, mu: {mu}\")\n","\n","                        # version 3\n","                        # global_models.append(local_models_data[local_machine])\n","                        global_distance, pairwise_distance = torch.norm(local_models_data[local_machine] - global_model_data), get_mu_pairwise_distance(global_models, MU_MULTIPLIER=MU_MULTIPLIER)\n","                        # global_models.pop()\n","                        # mu = torch.max(global_distance, pairwise_distance)\n","                        # mu = torch.min(global_distance, pairwise_distance)\n","                        # mu = global_distance * 0.5\n","                        # mu = (pairwise_distance + global_distance) / torch.tensor(2)\n","                        mu = pairwise_distance\n","                        print(f\"For round {epoch} and advNumber {local_machine}, mu: {mu}, global_distance: {global_distance}, pairwise_distance: {pairwise_distance}\")\n","\n","                    else:\n","                        mu = torch.tensor(1.0)\n","\n","                    delta = mu * deviation\n","\n","                    # print(f\"\\n\\nFor round {epoch} and advNumber {local_machine}, mu: {mu}\\ndeviation: {deviation}\\ndelta: {delta}\\nlocal model: {local_models_data[local_machine]}\\nglobal model: {global_model_data}\")\n","                    if epoch == 0:\n","                        print(f'local_models_data[local_machine] {local_models_data[local_machine].shape} delta {delta.shape}')\n","                        local_models_data[local_machine] -= delta\n","                    else:\n","                        # del local_models_data[local_machine]\n","                        local_models_data[local_machine] = global_model_data - delta\n","                    # print(f\"after update model after attack: {local_models_data[local_machine]}\\n\")\n","                    del deviation, delta, mu\n","\n","\n","            elif attack_type.startswith('VIRAT') and n_attackers > 0:\n","                local_models_data = virat_min_max(local_models_data, n_attackers, attack_type, epoch=epoch)\n","\n","            elif attack_type.startswith('LIE') and n_attackers > 0:\n","                local_models_data = lie_attack(local_models_data, n_attackers, z_values[(num_clients, n_attackers)])\n","\n","            elif attack_type =='FANG_TR_MEAN' and n_attackers > 0:\n","                local_models_data = fang_attack_trmean_partial(local_models_data, n_attackers)\n","\n","            elif attack_type =='FANG_KRUM' and n_attackers > 0:\n","                attacker_grads = torch.stack(local_models_data[:n_attackers])\n","                agg_grads = torch.mean(attacker_grads, 0)\n","                deviation = torch.sign(agg_grads)\n","                mal_update = get_malicious_updates_fang(attacker_grads, agg_grads, deviation, n_attackers)\n","                local_models_data = [mal_update] * n_attackers + local_models_data[n_attackers:]\n","\n","            elif attack_type.startswith('C_XFED'):\n","                local_models_data = xfed_c(local_models_data, n_attackers, attack_type, len(global_models), global_model_data, global_models, 0)\n","\n","            elif attack_type.startswith('Hybrid_XFED'):\n","                individual_attacker = n_attackers - collab\n","                for local_machine in range(individual_attacker):\n","                    if attack_type == 'Hybrid_XFED_unit_vec':\n","                        deviation = local_models_data[local_machine] / torch.norm(local_models_data[local_machine])\n","                    elif attack_type == 'Hybrid_XFED_sign':\n","                        sgn_vec = torch.sign(local_models_data[local_machine])\n","                        deviation = sgn_vec / torch.norm(sgn_vec)\n","                    else:\n","                        raise ValueError(\"Invalid attack type\")\n","\n","                    if len(global_models) > 1:\n","                        global_distance, pairwise_distance = torch.norm(local_models_data[local_machine] - global_model_data), get_mu_pairwise_distance(global_models, MU_MULTIPLIER=MU_MULTIPLIER)\n","                        mu = pairwise_distance\n","                        # print(f\"For round {epoch} and advNumber {local_machine}, mu: {mu}, global_distance: {global_distance}, pairwise_distance: {pairwise_distance}\")\n","\n","                    else:\n","                        mu = torch.tensor(1.0)\n","\n","                    delta = mu * deviation\n","\n","                    # print(f\"\\n\\nFor round {epoch} and advNumber {local_machine}, mu: {mu}\\ndeviation: {deviation}\\ndelta: {delta}\\nlocal model: {local_models_data[local_machine]}\\nglobal model: {global_model_data}\")\n","                    if epoch == 0:\n","                        # print(f'local_models_data[local_machine] {local_models_data[local_machine].shape} delta {delta.shape}')\n","                        local_models_data[local_machine] -= delta\n","                    else:\n","                        # del local_models_data[local_machine]\n","                        local_models_data[local_machine] = global_model_data - delta\n","                    # print(f\"after update model after attack: {local_models_data[local_machine]}\\n\")\n","                    del deviation, delta, mu\n","\n","                local_models_data = xfed_c(local_models_data, n_attackers, attack_type, len(global_models), global_model_data, global_models, collab)\n","\n","            else:\n","                raise ValueError(\"Invalid attack type\")\n","\n","            # print(f'For round {epoch}, attack done, Lenght of local_models_data:', len(local_models_data))\n","\n","\n","            if cross_device == True:\n","\n","                local_models_list = local_models_data.tolist()\n","                # Calculate the number of clients to select\n","                num_clients_to_select = max(1, int(num_clients * (20 / 100)))\n","                # Randomly select clients\n","                selected_clients_list = random.sample(local_models_list, num_clients_to_select)\n","                # Convert the selected list back to a tensor\n","                selected_clients_tensor = torch.tensor(selected_clients_list, device=local_models_data.device)\n","                local_models_data = selected_clients_tensor\n","                print(f'For round {epoch}, cross device done, Lenght of local_models_data:', len(local_models_data))\n","            else:\n","                pass\n","\n","            # Aggregate model updates\n","            if aggregation == 'MEAN':\n","                global_model_data = torch.mean(local_models_data, dim=0)\n","            elif aggregation == 'MEDIAN':\n","                global_model_data = torch.median(local_models_data, dim=0)[0]\n","            elif aggregation == 'KRUM':\n","                # Check if local_models_data is already a tensor\n","                if isinstance(local_models_data, list):\n","                    global_model_data, _ = multi_krum_optimized(local_updates=local_models_data)\n","                else:\n","                    global_model_data, _ = multi_krum_optimized(local_updates=local_models_data)\n","            elif aggregation == 'TR-MEAN':\n","                global_model_data = tr_mean(local_models_data)\n","            elif aggregation == 'CC':\n","                global_model_data = Clippedclustering(local_models_data)\n","            elif aggregation == 'SignGuard':\n","                global_model_data = SignGuard(local_models_data)\n","            else:\n","                raise ValueError(\"Invalid aggregation method\")\n","\n","            if torch.isnan(global_model_data).any():\n","                raise ValueError(\"NaN detected in model aggregation\")\n","\n","            # Update global model\n","            start_idx = 0\n","            with torch.no_grad():\n","                for param in global_model.parameters():\n","                    param_size = param.numel()\n","                    param.copy_(global_model_data[start_idx:start_idx + param_size].view(param.shape))\n","                    start_idx += param_size\n","\n","            # global_models.append(global_model_data.cpu())\n","            global_models.append(global_model_data)\n","\n","            print(f'For round {epoch}, aggregation done')\n","            last_ten_percent = int(n_round * 0.89)\n","            if epoch >= last_ten_percent or epoch%20 == 0:\n","                # Evaluate global model\n","                global_model.eval()\n","                global_model = global_model.to(device)\n","                correct = 0\n","                total = 0\n","                with torch.no_grad():\n","                    for images, labels in testloader:\n","                        images, labels = images.to(device), labels.to(device)\n","                        outputs = global_model(images)\n","                        _, predicted = torch.max(outputs.data, 1)\n","                        total += labels.size(0)\n","                        correct += (predicted == labels).sum().item()\n","\n","                accuracy = 100 * correct / total\n","                print(f'Time {datetime.now()}: Accuracy on round {epoch}, total {num_clients}, attackers {n_attackers}, attack_type {attack_type}, aggregation {aggregation} is: {accuracy:.2f} %')\n","\n","                # File path to save the accuracy log\n","                file_path = os.path.join(data_root, f'accuracy_{dataset}_{aggregation}_{attack_type}_{n_attackers}_mu{MU_MULTIPLIER}_cd_{cross_device}_collab{collab}_log.txt')\n","\n","                # Append accuracy to the file in the data_root location\n","                with open(file_path, 'a') as f:\n","                    f.write(f'Time {datetime.now()}: Accuracy on round {epoch}, dataset {dataset}, total {num_clients}, attackers {n_attackers}, attack_type {attack_type}, aggregation {aggregation} is: {accuracy:.2f} %\\n')\n","\n","            # global_model = global_model.to('cpu')\n","            del local_models_data\n","            torch.cuda.empty_cache()\n","            gc.collect()\n","\n","    # Final cleanup after training\n","    del global_model, train_data, testloader, global_models, criterion\n","\n"]},{"cell_type":"markdown","metadata":{"id":"NG0kwZK9lM9X"},"source":["Example Execution"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jnRgpMBTlSqv","executionInfo":{"status":"ok","timestamp":1736146630982,"user_tz":-360,"elapsed":509746,"user":{"displayName":"Israt Mouri","userId":"17729180794010364471"}},"outputId":"40847dbc-c875-430e-f0cc-c37b5f5d9340"},"outputs":[{"output_type":"stream","name":"stdout","text":["client_data_size 1578\n","For round 0, training done\n","For round 0, aggregation done\n","Time 2025-01-06 06:49:01.772434: Accuracy on round 0, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 3.24 %\n","For round 1, training done\n","For round 1, aggregation done\n","For round 2, training done\n","For round 2, aggregation done\n","For round 3, training done\n","For round 3, aggregation done\n","For round 4, training done\n","For round 4, aggregation done\n","For round 5, training done\n","For round 5, aggregation done\n","For round 6, training done\n","For round 6, aggregation done\n","For round 7, training done\n","For round 7, aggregation done\n","For round 8, training done\n","For round 8, aggregation done\n","For round 9, training done\n","For round 9, aggregation done\n","For round 10, training done\n","For round 10, aggregation done\n","For round 11, training done\n","For round 11, aggregation done\n","For round 12, training done\n","For round 12, aggregation done\n","For round 13, training done\n","For round 13, aggregation done\n","For round 14, training done\n","For round 14, aggregation done\n","For round 15, training done\n","For round 15, aggregation done\n","For round 16, training done\n","For round 16, aggregation done\n","For round 17, training done\n","For round 17, aggregation done\n","For round 18, training done\n","For round 18, aggregation done\n","For round 19, training done\n","For round 19, aggregation done\n","For round 20, training done\n","For round 20, aggregation done\n","Time 2025-01-06 06:49:19.511240: Accuracy on round 20, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 9.69 %\n","For round 21, training done\n","For round 21, aggregation done\n","For round 22, training done\n","For round 22, aggregation done\n","For round 23, training done\n","For round 23, aggregation done\n","For round 24, training done\n","For round 24, aggregation done\n","For round 25, training done\n","For round 25, aggregation done\n","For round 26, training done\n","For round 26, aggregation done\n","For round 27, training done\n","For round 27, aggregation done\n","For round 28, training done\n","For round 28, aggregation done\n","For round 29, training done\n","For round 29, aggregation done\n","For round 30, training done\n","For round 30, aggregation done\n","For round 31, training done\n","For round 31, aggregation done\n","For round 32, training done\n","For round 32, aggregation done\n","For round 33, training done\n","For round 33, aggregation done\n","For round 34, training done\n","For round 34, aggregation done\n","For round 35, training done\n","For round 35, aggregation done\n","For round 36, training done\n","For round 36, aggregation done\n","For round 37, training done\n","For round 37, aggregation done\n","For round 38, training done\n","For round 38, aggregation done\n","For round 39, training done\n","For round 39, aggregation done\n","For round 40, training done\n","For round 40, aggregation done\n","Time 2025-01-06 06:49:35.588475: Accuracy on round 40, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 22.43 %\n","For round 41, training done\n","For round 41, aggregation done\n","For round 42, training done\n","For round 42, aggregation done\n","For round 43, training done\n","For round 43, aggregation done\n","For round 44, training done\n","For round 44, aggregation done\n","For round 45, training done\n","For round 45, aggregation done\n","For round 46, training done\n","For round 46, aggregation done\n","For round 47, training done\n","For round 47, aggregation done\n","For round 48, training done\n","For round 48, aggregation done\n","For round 49, training done\n","For round 49, aggregation done\n","For round 50, training done\n","For round 50, aggregation done\n","For round 51, training done\n","For round 51, aggregation done\n","For round 52, training done\n","For round 52, aggregation done\n","For round 53, training done\n","For round 53, aggregation done\n","For round 54, training done\n","For round 54, aggregation done\n","For round 55, training done\n","For round 55, aggregation done\n","For round 56, training done\n","For round 56, aggregation done\n","For round 57, training done\n","For round 57, aggregation done\n","For round 58, training done\n","For round 58, aggregation done\n","For round 59, training done\n","For round 59, aggregation done\n","For round 60, training done\n","For round 60, aggregation done\n","Time 2025-01-06 06:49:51.871557: Accuracy on round 60, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 23.22 %\n","For round 61, training done\n","For round 61, aggregation done\n","For round 62, training done\n","For round 62, aggregation done\n","For round 63, training done\n","For round 63, aggregation done\n","For round 64, training done\n","For round 64, aggregation done\n","For round 65, training done\n","For round 65, aggregation done\n","For round 66, training done\n","For round 66, aggregation done\n","For round 67, training done\n","For round 67, aggregation done\n","For round 68, training done\n","For round 68, aggregation done\n","For round 69, training done\n","For round 69, aggregation done\n","For round 70, training done\n","For round 70, aggregation done\n","For round 71, training done\n","For round 71, aggregation done\n","For round 72, training done\n","For round 72, aggregation done\n","For round 73, training done\n","For round 73, aggregation done\n","For round 74, training done\n","For round 74, aggregation done\n","For round 75, training done\n","For round 75, aggregation done\n","For round 76, training done\n","For round 76, aggregation done\n","For round 77, training done\n","For round 77, aggregation done\n","For round 78, training done\n","For round 78, aggregation done\n","For round 79, training done\n","For round 79, aggregation done\n","For round 80, training done\n","For round 80, aggregation done\n","Time 2025-01-06 06:50:08.194107: Accuracy on round 80, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 32.01 %\n","For round 81, training done\n","For round 81, aggregation done\n","For round 82, training done\n","For round 82, aggregation done\n","For round 83, training done\n","For round 83, aggregation done\n","For round 84, training done\n","For round 84, aggregation done\n","For round 85, training done\n","For round 85, aggregation done\n","For round 86, training done\n","For round 86, aggregation done\n","For round 87, training done\n","For round 87, aggregation done\n","For round 88, training done\n","For round 88, aggregation done\n","For round 89, training done\n","For round 89, aggregation done\n","For round 90, training done\n","For round 90, aggregation done\n","For round 91, training done\n","For round 91, aggregation done\n","For round 92, training done\n","For round 92, aggregation done\n","For round 93, training done\n","For round 93, aggregation done\n","For round 94, training done\n","For round 94, aggregation done\n","For round 95, training done\n","For round 95, aggregation done\n","For round 96, training done\n","For round 96, aggregation done\n","For round 97, training done\n","For round 97, aggregation done\n","For round 98, training done\n","For round 98, aggregation done\n","For round 99, training done\n","For round 99, aggregation done\n","For round 100, training done\n","For round 100, aggregation done\n","Time 2025-01-06 06:50:24.356207: Accuracy on round 100, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 32.03 %\n","For round 101, training done\n","For round 101, aggregation done\n","For round 102, training done\n","For round 102, aggregation done\n","For round 103, training done\n","For round 103, aggregation done\n","For round 104, training done\n","For round 104, aggregation done\n","For round 105, training done\n","For round 105, aggregation done\n","For round 106, training done\n","For round 106, aggregation done\n","For round 107, training done\n","For round 107, aggregation done\n","For round 108, training done\n","For round 108, aggregation done\n","For round 109, training done\n","For round 109, aggregation done\n","For round 110, training done\n","For round 110, aggregation done\n","For round 111, training done\n","For round 111, aggregation done\n","For round 112, training done\n","For round 112, aggregation done\n","For round 113, training done\n","For round 113, aggregation done\n","For round 114, training done\n","For round 114, aggregation done\n","For round 115, training done\n","For round 115, aggregation done\n","For round 116, training done\n","For round 116, aggregation done\n","For round 117, training done\n","For round 117, aggregation done\n","For round 118, training done\n","For round 118, aggregation done\n","For round 119, training done\n","For round 119, aggregation done\n","For round 120, training done\n","For round 120, aggregation done\n","Time 2025-01-06 06:50:40.499886: Accuracy on round 120, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 33.53 %\n","For round 121, training done\n","For round 121, aggregation done\n","For round 122, training done\n","For round 122, aggregation done\n","For round 123, training done\n","For round 123, aggregation done\n","For round 124, training done\n","For round 124, aggregation done\n","For round 125, training done\n","For round 125, aggregation done\n","For round 126, training done\n","For round 126, aggregation done\n","For round 127, training done\n","For round 127, aggregation done\n","For round 128, training done\n","For round 128, aggregation done\n","For round 129, training done\n","For round 129, aggregation done\n","For round 130, training done\n","For round 130, aggregation done\n","For round 131, training done\n","For round 131, aggregation done\n","For round 132, training done\n","For round 132, aggregation done\n","For round 133, training done\n","For round 133, aggregation done\n","For round 134, training done\n","For round 134, aggregation done\n","For round 135, training done\n","For round 135, aggregation done\n","For round 136, training done\n","For round 136, aggregation done\n","For round 137, training done\n","For round 137, aggregation done\n","For round 138, training done\n","For round 138, aggregation done\n","For round 139, training done\n","For round 139, aggregation done\n","For round 140, training done\n","For round 140, aggregation done\n","Time 2025-01-06 06:50:56.650097: Accuracy on round 140, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 38.56 %\n","For round 141, training done\n","For round 141, aggregation done\n","For round 142, training done\n","For round 142, aggregation done\n","For round 143, training done\n","For round 143, aggregation done\n","For round 144, training done\n","For round 144, aggregation done\n","For round 145, training done\n","For round 145, aggregation done\n","For round 146, training done\n","For round 146, aggregation done\n","For round 147, training done\n","For round 147, aggregation done\n","For round 148, training done\n","For round 148, aggregation done\n","For round 149, training done\n","For round 149, aggregation done\n","For round 150, training done\n","For round 150, aggregation done\n","For round 151, training done\n","For round 151, aggregation done\n","For round 152, training done\n","For round 152, aggregation done\n","For round 153, training done\n","For round 153, aggregation done\n","For round 154, training done\n","For round 154, aggregation done\n","For round 155, training done\n","For round 155, aggregation done\n","For round 156, training done\n","For round 156, aggregation done\n","For round 157, training done\n","For round 157, aggregation done\n","For round 158, training done\n","For round 158, aggregation done\n","For round 159, training done\n","For round 159, aggregation done\n","For round 160, training done\n","For round 160, aggregation done\n","Time 2025-01-06 06:51:12.825031: Accuracy on round 160, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 42.38 %\n","For round 161, training done\n","For round 161, aggregation done\n","For round 162, training done\n","For round 162, aggregation done\n","For round 163, training done\n","For round 163, aggregation done\n","For round 164, training done\n","For round 164, aggregation done\n","For round 165, training done\n","For round 165, aggregation done\n","For round 166, training done\n","For round 166, aggregation done\n","For round 167, training done\n","For round 167, aggregation done\n","For round 168, training done\n","For round 168, aggregation done\n","For round 169, training done\n","For round 169, aggregation done\n","For round 170, training done\n","For round 170, aggregation done\n","For round 171, training done\n","For round 171, aggregation done\n","For round 172, training done\n","For round 172, aggregation done\n","For round 173, training done\n","For round 173, aggregation done\n","For round 174, training done\n","For round 174, aggregation done\n","For round 175, training done\n","For round 175, aggregation done\n","For round 176, training done\n","For round 176, aggregation done\n","For round 177, training done\n","For round 177, aggregation done\n","For round 178, training done\n","For round 178, aggregation done\n","For round 179, training done\n","For round 179, aggregation done\n","For round 180, training done\n","For round 180, aggregation done\n","Time 2025-01-06 06:51:29.014720: Accuracy on round 180, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 47.21 %\n","For round 181, training done\n","For round 181, aggregation done\n","For round 182, training done\n","For round 182, aggregation done\n","For round 183, training done\n","For round 183, aggregation done\n","For round 184, training done\n","For round 184, aggregation done\n","For round 185, training done\n","For round 185, aggregation done\n","For round 186, training done\n","For round 186, aggregation done\n","For round 187, training done\n","For round 187, aggregation done\n","For round 188, training done\n","For round 188, aggregation done\n","For round 189, training done\n","For round 189, aggregation done\n","For round 190, training done\n","For round 190, aggregation done\n","For round 191, training done\n","For round 191, aggregation done\n","For round 192, training done\n","For round 192, aggregation done\n","For round 193, training done\n","For round 193, aggregation done\n","For round 194, training done\n","For round 194, aggregation done\n","For round 195, training done\n","For round 195, aggregation done\n","For round 196, training done\n","For round 196, aggregation done\n","For round 197, training done\n","For round 197, aggregation done\n","For round 198, training done\n","For round 198, aggregation done\n","For round 199, training done\n","For round 199, aggregation done\n","For round 200, training done\n","For round 200, aggregation done\n","Time 2025-01-06 06:51:45.285766: Accuracy on round 200, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 37.06 %\n","For round 201, training done\n","For round 201, aggregation done\n","For round 202, training done\n","For round 202, aggregation done\n","For round 203, training done\n","For round 203, aggregation done\n","For round 204, training done\n","For round 204, aggregation done\n","For round 205, training done\n","For round 205, aggregation done\n","For round 206, training done\n","For round 206, aggregation done\n","For round 207, training done\n","For round 207, aggregation done\n","For round 208, training done\n","For round 208, aggregation done\n","For round 209, training done\n","For round 209, aggregation done\n","For round 210, training done\n","For round 210, aggregation done\n","For round 211, training done\n","For round 211, aggregation done\n","For round 212, training done\n","For round 212, aggregation done\n","For round 213, training done\n","For round 213, aggregation done\n","For round 214, training done\n","For round 214, aggregation done\n","For round 215, training done\n","For round 215, aggregation done\n","For round 216, training done\n","For round 216, aggregation done\n","For round 217, training done\n","For round 217, aggregation done\n","For round 218, training done\n","For round 218, aggregation done\n","For round 219, training done\n","For round 219, aggregation done\n","For round 220, training done\n","For round 220, aggregation done\n","Time 2025-01-06 06:52:01.545535: Accuracy on round 220, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 34.91 %\n","For round 221, training done\n","For round 221, aggregation done\n","For round 222, training done\n","For round 222, aggregation done\n","For round 223, training done\n","For round 223, aggregation done\n","For round 224, training done\n","For round 224, aggregation done\n","For round 225, training done\n","For round 225, aggregation done\n","For round 226, training done\n","For round 226, aggregation done\n","For round 227, training done\n","For round 227, aggregation done\n","For round 228, training done\n","For round 228, aggregation done\n","For round 229, training done\n","For round 229, aggregation done\n","For round 230, training done\n","For round 230, aggregation done\n","For round 231, training done\n","For round 231, aggregation done\n","For round 232, training done\n","For round 232, aggregation done\n","For round 233, training done\n","For round 233, aggregation done\n","For round 234, training done\n","For round 234, aggregation done\n","For round 235, training done\n","For round 235, aggregation done\n","For round 236, training done\n","For round 236, aggregation done\n","For round 237, training done\n","For round 237, aggregation done\n","For round 238, training done\n","For round 238, aggregation done\n","For round 239, training done\n","For round 239, aggregation done\n","For round 240, training done\n","For round 240, aggregation done\n","Time 2025-01-06 06:52:17.937147: Accuracy on round 240, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 41.75 %\n","For round 241, training done\n","For round 241, aggregation done\n","For round 242, training done\n","For round 242, aggregation done\n","For round 243, training done\n","For round 243, aggregation done\n","For round 244, training done\n","For round 244, aggregation done\n","For round 245, training done\n","For round 245, aggregation done\n","For round 246, training done\n","For round 246, aggregation done\n","For round 247, training done\n","For round 247, aggregation done\n","For round 248, training done\n","For round 248, aggregation done\n","For round 249, training done\n","For round 249, aggregation done\n","For round 250, training done\n","For round 250, aggregation done\n","For round 251, training done\n","For round 251, aggregation done\n","For round 252, training done\n","For round 252, aggregation done\n","For round 253, training done\n","For round 253, aggregation done\n","For round 254, training done\n","For round 254, aggregation done\n","For round 255, training done\n","For round 255, aggregation done\n","For round 256, training done\n","For round 256, aggregation done\n","For round 257, training done\n","For round 257, aggregation done\n","For round 258, training done\n","For round 258, aggregation done\n","For round 259, training done\n","For round 259, aggregation done\n","For round 260, training done\n","For round 260, aggregation done\n","Time 2025-01-06 06:52:34.016214: Accuracy on round 260, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 42.31 %\n","For round 261, training done\n","For round 261, aggregation done\n","For round 262, training done\n","For round 262, aggregation done\n","For round 263, training done\n","For round 263, aggregation done\n","For round 264, training done\n","For round 264, aggregation done\n","For round 265, training done\n","For round 265, aggregation done\n","For round 266, training done\n","For round 266, aggregation done\n","For round 267, training done\n","For round 267, aggregation done\n","For round 268, training done\n","For round 268, aggregation done\n","For round 269, training done\n","For round 269, aggregation done\n","For round 270, training done\n","For round 270, aggregation done\n","For round 271, training done\n","For round 271, aggregation done\n","For round 272, training done\n","For round 272, aggregation done\n","For round 273, training done\n","For round 273, aggregation done\n","For round 274, training done\n","For round 274, aggregation done\n","For round 275, training done\n","For round 275, aggregation done\n","For round 276, training done\n","For round 276, aggregation done\n","For round 277, training done\n","For round 277, aggregation done\n","For round 278, training done\n","For round 278, aggregation done\n","For round 279, training done\n","For round 279, aggregation done\n","For round 280, training done\n","For round 280, aggregation done\n","Time 2025-01-06 06:52:50.184030: Accuracy on round 280, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 44.37 %\n","For round 281, training done\n","For round 281, aggregation done\n","For round 282, training done\n","For round 282, aggregation done\n","For round 283, training done\n","For round 283, aggregation done\n","For round 284, training done\n","For round 284, aggregation done\n","For round 285, training done\n","For round 285, aggregation done\n","For round 286, training done\n","For round 286, aggregation done\n","For round 287, training done\n","For round 287, aggregation done\n","For round 288, training done\n","For round 288, aggregation done\n","For round 289, training done\n","For round 289, aggregation done\n","For round 290, training done\n","For round 290, aggregation done\n","For round 291, training done\n","For round 291, aggregation done\n","For round 292, training done\n","For round 292, aggregation done\n","For round 293, training done\n","For round 293, aggregation done\n","For round 294, training done\n","For round 294, aggregation done\n","For round 295, training done\n","For round 295, aggregation done\n","For round 296, training done\n","For round 296, aggregation done\n","For round 297, training done\n","For round 297, aggregation done\n","For round 298, training done\n","For round 298, aggregation done\n","For round 299, training done\n","For round 299, aggregation done\n","For round 300, training done\n","For round 300, aggregation done\n","Time 2025-01-06 06:53:06.279084: Accuracy on round 300, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 44.06 %\n","For round 301, training done\n","For round 301, aggregation done\n","For round 302, training done\n","For round 302, aggregation done\n","For round 303, training done\n","For round 303, aggregation done\n","For round 304, training done\n","For round 304, aggregation done\n","For round 305, training done\n","For round 305, aggregation done\n","For round 306, training done\n","For round 306, aggregation done\n","For round 307, training done\n","For round 307, aggregation done\n","For round 308, training done\n","For round 308, aggregation done\n","For round 309, training done\n","For round 309, aggregation done\n","For round 310, training done\n","For round 310, aggregation done\n","For round 311, training done\n","For round 311, aggregation done\n","For round 312, training done\n","For round 312, aggregation done\n","For round 313, training done\n","For round 313, aggregation done\n","For round 314, training done\n","For round 314, aggregation done\n","For round 315, training done\n","For round 315, aggregation done\n","For round 316, training done\n","For round 316, aggregation done\n","For round 317, training done\n","For round 317, aggregation done\n","For round 318, training done\n","For round 318, aggregation done\n","For round 319, training done\n","For round 319, aggregation done\n","For round 320, training done\n","For round 320, aggregation done\n","Time 2025-01-06 06:53:22.421146: Accuracy on round 320, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 37.30 %\n","For round 321, training done\n","For round 321, aggregation done\n","For round 322, training done\n","For round 322, aggregation done\n","For round 323, training done\n","For round 323, aggregation done\n","For round 324, training done\n","For round 324, aggregation done\n","For round 325, training done\n","For round 325, aggregation done\n","For round 326, training done\n","For round 326, aggregation done\n","For round 327, training done\n","For round 327, aggregation done\n","For round 328, training done\n","For round 328, aggregation done\n","For round 329, training done\n","For round 329, aggregation done\n","For round 330, training done\n","For round 330, aggregation done\n","For round 331, training done\n","For round 331, aggregation done\n","For round 332, training done\n","For round 332, aggregation done\n","For round 333, training done\n","For round 333, aggregation done\n","For round 334, training done\n","For round 334, aggregation done\n","For round 335, training done\n","For round 335, aggregation done\n","For round 336, training done\n","For round 336, aggregation done\n","For round 337, training done\n","For round 337, aggregation done\n","For round 338, training done\n","For round 338, aggregation done\n","For round 339, training done\n","For round 339, aggregation done\n","For round 340, training done\n","For round 340, aggregation done\n","Time 2025-01-06 06:53:38.609010: Accuracy on round 340, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 39.52 %\n","For round 341, training done\n","For round 341, aggregation done\n","For round 342, training done\n","For round 342, aggregation done\n","For round 343, training done\n","For round 343, aggregation done\n","For round 344, training done\n","For round 344, aggregation done\n","For round 345, training done\n","For round 345, aggregation done\n","For round 346, training done\n","For round 346, aggregation done\n","For round 347, training done\n","For round 347, aggregation done\n","For round 348, training done\n","For round 348, aggregation done\n","For round 349, training done\n","For round 349, aggregation done\n","For round 350, training done\n","For round 350, aggregation done\n","For round 351, training done\n","For round 351, aggregation done\n","For round 352, training done\n","For round 352, aggregation done\n","For round 353, training done\n","For round 353, aggregation done\n","For round 354, training done\n","For round 354, aggregation done\n","For round 355, training done\n","For round 355, aggregation done\n","For round 356, training done\n","For round 356, aggregation done\n","For round 357, training done\n","For round 357, aggregation done\n","For round 358, training done\n","For round 358, aggregation done\n","For round 359, training done\n","For round 359, aggregation done\n","For round 360, training done\n","For round 360, aggregation done\n","Time 2025-01-06 06:53:54.789358: Accuracy on round 360, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 39.38 %\n","For round 361, training done\n","For round 361, aggregation done\n","For round 362, training done\n","For round 362, aggregation done\n","For round 363, training done\n","For round 363, aggregation done\n","For round 364, training done\n","For round 364, aggregation done\n","For round 365, training done\n","For round 365, aggregation done\n","For round 366, training done\n","For round 366, aggregation done\n","For round 367, training done\n","For round 367, aggregation done\n","For round 368, training done\n","For round 368, aggregation done\n","For round 369, training done\n","For round 369, aggregation done\n","For round 370, training done\n","For round 370, aggregation done\n","For round 371, training done\n","For round 371, aggregation done\n","For round 372, training done\n","For round 372, aggregation done\n","For round 373, training done\n","For round 373, aggregation done\n","For round 374, training done\n","For round 374, aggregation done\n","For round 375, training done\n","For round 375, aggregation done\n","For round 376, training done\n","For round 376, aggregation done\n","For round 377, training done\n","For round 377, aggregation done\n","For round 378, training done\n","For round 378, aggregation done\n","For round 379, training done\n","For round 379, aggregation done\n","For round 380, training done\n","For round 380, aggregation done\n","Time 2025-01-06 06:54:10.962500: Accuracy on round 380, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 37.70 %\n","For round 381, training done\n","For round 381, aggregation done\n","For round 382, training done\n","For round 382, aggregation done\n","For round 383, training done\n","For round 383, aggregation done\n","For round 384, training done\n","For round 384, aggregation done\n","For round 385, training done\n","For round 385, aggregation done\n","For round 386, training done\n","For round 386, aggregation done\n","For round 387, training done\n","For round 387, aggregation done\n","For round 388, training done\n","For round 388, aggregation done\n","For round 389, training done\n","For round 389, aggregation done\n","For round 390, training done\n","For round 390, aggregation done\n","For round 391, training done\n","For round 391, aggregation done\n","For round 392, training done\n","For round 392, aggregation done\n","For round 393, training done\n","For round 393, aggregation done\n","For round 394, training done\n","For round 394, aggregation done\n","For round 395, training done\n","For round 395, aggregation done\n","For round 396, training done\n","For round 396, aggregation done\n","For round 397, training done\n","For round 397, aggregation done\n","For round 398, training done\n","For round 398, aggregation done\n","For round 399, training done\n","For round 399, aggregation done\n","For round 400, training done\n","For round 400, aggregation done\n","Time 2025-01-06 06:54:27.371279: Accuracy on round 400, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 36.01 %\n","For round 401, training done\n","For round 401, aggregation done\n","For round 402, training done\n","For round 402, aggregation done\n","For round 403, training done\n","For round 403, aggregation done\n","For round 404, training done\n","For round 404, aggregation done\n","For round 405, training done\n","For round 405, aggregation done\n","For round 406, training done\n","For round 406, aggregation done\n","For round 407, training done\n","For round 407, aggregation done\n","For round 408, training done\n","For round 408, aggregation done\n","For round 409, training done\n","For round 409, aggregation done\n","For round 410, training done\n","For round 410, aggregation done\n","For round 411, training done\n","For round 411, aggregation done\n","For round 412, training done\n","For round 412, aggregation done\n","For round 413, training done\n","For round 413, aggregation done\n","For round 414, training done\n","For round 414, aggregation done\n","For round 415, training done\n","For round 415, aggregation done\n","For round 416, training done\n","For round 416, aggregation done\n","For round 417, training done\n","For round 417, aggregation done\n","For round 418, training done\n","For round 418, aggregation done\n","For round 419, training done\n","For round 419, aggregation done\n","For round 420, training done\n","For round 420, aggregation done\n","Time 2025-01-06 06:54:43.590045: Accuracy on round 420, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 36.00 %\n","For round 421, training done\n","For round 421, aggregation done\n","For round 422, training done\n","For round 422, aggregation done\n","For round 423, training done\n","For round 423, aggregation done\n","For round 424, training done\n","For round 424, aggregation done\n","For round 425, training done\n","For round 425, aggregation done\n","For round 426, training done\n","For round 426, aggregation done\n","For round 427, training done\n","For round 427, aggregation done\n","For round 428, training done\n","For round 428, aggregation done\n","For round 429, training done\n","For round 429, aggregation done\n","For round 430, training done\n","For round 430, aggregation done\n","For round 431, training done\n","For round 431, aggregation done\n","For round 432, training done\n","For round 432, aggregation done\n","For round 433, training done\n","For round 433, aggregation done\n","For round 434, training done\n","For round 434, aggregation done\n","For round 435, training done\n","For round 435, aggregation done\n","For round 436, training done\n","For round 436, aggregation done\n","For round 437, training done\n","For round 437, aggregation done\n","For round 438, training done\n","For round 438, aggregation done\n","For round 439, training done\n","For round 439, aggregation done\n","For round 440, training done\n","For round 440, aggregation done\n","Time 2025-01-06 06:54:59.744354: Accuracy on round 440, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 38.86 %\n","For round 441, training done\n","For round 441, aggregation done\n","For round 442, training done\n","For round 442, aggregation done\n","For round 443, training done\n","For round 443, aggregation done\n","For round 444, training done\n","For round 444, aggregation done\n","For round 445, training done\n","For round 445, aggregation done\n","Time 2025-01-06 06:55:05.047384: Accuracy on round 445, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 36.39 %\n","For round 446, training done\n","For round 446, aggregation done\n","Time 2025-01-06 06:55:07.336128: Accuracy on round 446, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 36.39 %\n","For round 447, training done\n","For round 447, aggregation done\n","Time 2025-01-06 06:55:09.593432: Accuracy on round 447, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 32.69 %\n","For round 448, training done\n","For round 448, aggregation done\n","Time 2025-01-06 06:55:11.848446: Accuracy on round 448, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 36.02 %\n","For round 449, training done\n","For round 449, aggregation done\n","Time 2025-01-06 06:55:14.188140: Accuracy on round 449, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 36.60 %\n","For round 450, training done\n","For round 450, aggregation done\n","Time 2025-01-06 06:55:16.531369: Accuracy on round 450, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 39.46 %\n","For round 451, training done\n","For round 451, aggregation done\n","Time 2025-01-06 06:55:18.817917: Accuracy on round 451, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 37.00 %\n","For round 452, training done\n","For round 452, aggregation done\n","Time 2025-01-06 06:55:21.178168: Accuracy on round 452, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 38.06 %\n","For round 453, training done\n","For round 453, aggregation done\n","Time 2025-01-06 06:55:23.446525: Accuracy on round 453, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 31.51 %\n","For round 454, training done\n","For round 454, aggregation done\n","Time 2025-01-06 06:55:25.737382: Accuracy on round 454, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 38.90 %\n","For round 455, training done\n","For round 455, aggregation done\n","Time 2025-01-06 06:55:28.080883: Accuracy on round 455, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 35.06 %\n","For round 456, training done\n","For round 456, aggregation done\n","Time 2025-01-06 06:55:30.442947: Accuracy on round 456, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 41.23 %\n","For round 457, training done\n","For round 457, aggregation done\n","Time 2025-01-06 06:55:32.743165: Accuracy on round 457, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 36.90 %\n","For round 458, training done\n","For round 458, aggregation done\n","Time 2025-01-06 06:55:35.051474: Accuracy on round 458, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 40.78 %\n","For round 459, training done\n","For round 459, aggregation done\n","Time 2025-01-06 06:55:37.308749: Accuracy on round 459, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 33.28 %\n","For round 460, training done\n","For round 460, aggregation done\n","Time 2025-01-06 06:55:39.587985: Accuracy on round 460, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 39.91 %\n","For round 461, training done\n","For round 461, aggregation done\n","Time 2025-01-06 06:55:41.945409: Accuracy on round 461, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 35.45 %\n","For round 462, training done\n","For round 462, aggregation done\n","Time 2025-01-06 06:55:44.297867: Accuracy on round 462, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 41.61 %\n","For round 463, training done\n","For round 463, aggregation done\n","Time 2025-01-06 06:55:46.542559: Accuracy on round 463, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 37.66 %\n","For round 464, training done\n","For round 464, aggregation done\n","Time 2025-01-06 06:55:48.805003: Accuracy on round 464, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 42.82 %\n","For round 465, training done\n","For round 465, aggregation done\n","Time 2025-01-06 06:55:51.166049: Accuracy on round 465, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 30.61 %\n","For round 466, training done\n","For round 466, aggregation done\n","Time 2025-01-06 06:55:53.471165: Accuracy on round 466, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 37.23 %\n","For round 467, training done\n","For round 467, aggregation done\n","Time 2025-01-06 06:55:55.804080: Accuracy on round 467, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 29.78 %\n","For round 468, training done\n","For round 468, aggregation done\n","Time 2025-01-06 06:55:58.054879: Accuracy on round 468, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 40.85 %\n","For round 469, training done\n","For round 469, aggregation done\n","Time 2025-01-06 06:56:00.407655: Accuracy on round 469, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 31.68 %\n","For round 470, training done\n","For round 470, aggregation done\n","Time 2025-01-06 06:56:02.758460: Accuracy on round 470, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 40.08 %\n","For round 471, training done\n","For round 471, aggregation done\n","Time 2025-01-06 06:56:05.049213: Accuracy on round 471, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 32.75 %\n","For round 472, training done\n","For round 472, aggregation done\n","Time 2025-01-06 06:56:07.373625: Accuracy on round 472, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 35.19 %\n","For round 473, training done\n","For round 473, aggregation done\n","Time 2025-01-06 06:56:09.668559: Accuracy on round 473, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 36.77 %\n","For round 474, training done\n","For round 474, aggregation done\n","Time 2025-01-06 06:56:11.993617: Accuracy on round 474, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 36.83 %\n","For round 475, training done\n","For round 475, aggregation done\n","Time 2025-01-06 06:56:14.299857: Accuracy on round 475, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 40.97 %\n","For round 476, training done\n","For round 476, aggregation done\n","Time 2025-01-06 06:56:16.536699: Accuracy on round 476, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 36.15 %\n","For round 477, training done\n","For round 477, aggregation done\n","Time 2025-01-06 06:56:18.887663: Accuracy on round 477, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 37.45 %\n","For round 478, training done\n","For round 478, aggregation done\n","Time 2025-01-06 06:56:21.343540: Accuracy on round 478, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 34.26 %\n","For round 479, training done\n","For round 479, aggregation done\n","Time 2025-01-06 06:56:23.726241: Accuracy on round 479, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 37.72 %\n","For round 480, training done\n","For round 480, aggregation done\n","Time 2025-01-06 06:56:26.085958: Accuracy on round 480, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 34.94 %\n","For round 481, training done\n","For round 481, aggregation done\n","Time 2025-01-06 06:56:28.419350: Accuracy on round 481, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 35.87 %\n","For round 482, training done\n","For round 482, aggregation done\n","Time 2025-01-06 06:56:30.714636: Accuracy on round 482, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 35.75 %\n","For round 483, training done\n","For round 483, aggregation done\n","Time 2025-01-06 06:56:33.044438: Accuracy on round 483, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 30.16 %\n","For round 484, training done\n","For round 484, aggregation done\n","Time 2025-01-06 06:56:35.354799: Accuracy on round 484, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 35.90 %\n","For round 485, training done\n","For round 485, aggregation done\n","Time 2025-01-06 06:56:37.653927: Accuracy on round 485, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 33.43 %\n","For round 486, training done\n","For round 486, aggregation done\n","Time 2025-01-06 06:56:39.991591: Accuracy on round 486, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 39.01 %\n","For round 487, training done\n","For round 487, aggregation done\n","Time 2025-01-06 06:56:42.295464: Accuracy on round 487, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 37.28 %\n","For round 488, training done\n","For round 488, aggregation done\n","Time 2025-01-06 06:56:44.604152: Accuracy on round 488, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 38.73 %\n","For round 489, training done\n","For round 489, aggregation done\n","Time 2025-01-06 06:56:46.992576: Accuracy on round 489, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 37.78 %\n","For round 490, training done\n","For round 490, aggregation done\n","Time 2025-01-06 06:56:49.259690: Accuracy on round 490, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 35.12 %\n","For round 491, training done\n","For round 491, aggregation done\n","Time 2025-01-06 06:56:51.566742: Accuracy on round 491, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 35.37 %\n","For round 492, training done\n","For round 492, aggregation done\n","Time 2025-01-06 06:56:53.886529: Accuracy on round 492, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 32.86 %\n","For round 493, training done\n","For round 493, aggregation done\n","Time 2025-01-06 06:56:56.223354: Accuracy on round 493, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 38.21 %\n","For round 494, training done\n","For round 494, aggregation done\n","Time 2025-01-06 06:56:58.533477: Accuracy on round 494, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 31.85 %\n","For round 495, training done\n","For round 495, aggregation done\n","Time 2025-01-06 06:57:00.904698: Accuracy on round 495, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 39.52 %\n","For round 496, training done\n","For round 496, aggregation done\n","Time 2025-01-06 06:57:03.268557: Accuracy on round 496, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 32.64 %\n","For round 497, training done\n","For round 497, aggregation done\n","Time 2025-01-06 06:57:05.593269: Accuracy on round 497, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 38.62 %\n","For round 498, training done\n","For round 498, aggregation done\n","Time 2025-01-06 06:57:07.892043: Accuracy on round 498, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 33.01 %\n","For round 499, training done\n","For round 499, aggregation done\n","Time 2025-01-06 06:57:10.216107: Accuracy on round 499, total 100, attackers 20, attack_type C_XFED_sign, aggregation TR-MEAN is: 41.20 %\n"]}],"source":["# Example execution\n","for MU_MULTIPLIER in [5]\n","for cb in [0]:\n","    for attack_type in ['FANG_KRUM']: # 'Hybrid_XFED_unit_vec','Hybrid_XFED_sign'  'XFED_unit_vec', 'XFED_sign', 'VIRAT_unit_vec', 'C_XFED_sign', 'LIE', 'FANG_TR_MEAN'\n","        for agg in ['KRUM']: # 'MEAN', 'MEDIAN', 'KRUM', 'TR-MEAN', 'SignGuard', 'CC' 'MEAN', 'MEDIAN', 'KRUM', 'TR-MEAN', 'SignGuard'\n","            for attackers in [20]:\n","                # torch.cuda.memory._record_memory_history()\n","                # federated_learning(num_clients=40, n_attackers=attackers, aggregation=agg, n_round=300, dataset='EMNIST', attack_type=attack_type, batch_size=256, optim=\"SGD\")\n","                # federated_learning(num_clients=40, n_attackers=attackers, aggregation=agg, n_round=250, dataset='FashionMNIST', attack_type=attack_type, batch_size=256, optim=\"SGD\")\n","                # federated_learning(num_clients=50, n_attackers=attackers, aggregation=agg, n_round=255, dataset='CIFAR10', attack_type=attack_type, batch_size=250)\n","                # federated_learning(num_clients=100, n_attackers=attackers, aggregation=agg, n_round=1000, dataset='SVHN', attack_type=attack_type, batch_size=64, optim=\"SGD\")\n","                # federated_learning(num_clients=100, n_attackers=attackers, aggregation=agg, n_round=275, dataset='MNIST', attack_type=attack_type, batch_size=256, optim=\"SGD\")\n","                # federated_learning(num_clients=100, n_attackers=attackers, aggregation=agg, n_round=500, dataset='PURCHASE', attack_type=attack_type, batch_size=128, optim=\"SGD\", cross_device=False, collab=cb)\n","                # federated_learning(num_clients=40, n_attackers=attackers, aggregation=agg, n_round=300, dataset='CIFAR100', attack_type=attack_type, batch_size=250, optim=\"Adam\")\n","                # federated_learning(num_clients=200, n_attackers=attackers, aggregation=agg, n_round=50, dataset='SYMBIPREDICT', attack_type=attack_type, batch_size=10, optim=\"SGD\", collab=0)\n","                # federated_learning(num_clients=200, n_attackers=attackers, aggregation=agg, n_round=50, dataset='SYMBIPREDICT', attack_type=attack_type, batch_size=10, optim=\"Adam\", cross_device=False, collab=0)\n","                # torch.cuda.memory._dump_snapshot(\"cifar10.pickle\")\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[{"file_id":"1X3t5mq5GeYar0SJTQdAWqgIr9zVQTsLp","timestamp":1736246944293},{"file_id":"1zDVH8FKTgN-bePGXWP0Hy_EIVOkPG2KF","timestamp":1730633766005}],"authorship_tag":"ABX9TyPExLU/T0qmqKUeWyvYqieW"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}